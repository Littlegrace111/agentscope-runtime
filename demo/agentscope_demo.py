import os
from contextlib import asynccontextmanager
from agentscope_runtime.engine import Runner
from agentscope_runtime.engine.agents.llm_agent import LLMAgent
from agentscope_runtime.engine.llms import QwenLLM
from agentscope_runtime.engine.schemas.agent_schemas import (
    MessageType,
    RunStatus,
    AgentRequest,
)
from agentscope_runtime.engine.services.context_manager import (
    ContextManager,
)

print("âœ… ä¾èµ–å¯¼å…¥æˆåŠŸ")


from agentscope.agent import ReActAgent
from agentscope.model import OpenAIChatModel
from agentscope_runtime.engine.agents.agentscope_agent import AgentScopeAgent


# åˆ›å»ºLLMå®ä¾‹
model = QwenLLM(
    model_name="qwen-turbo",
    api_key=os.getenv("DASHSCOPE_API_KEY")
)

# åˆ›å»ºLLMæ™ºèƒ½ä½“
llm_agent = LLMAgent(
    model=model,
    name="llm_agent",
    description="A simple LLM agent for text generation",
)

print("âœ… LLMæ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")

agent = AgentScopeAgent(
    name="Friday",
    model=OpenAIChatModel(
        "gpt-4",
        api_key=os.getenv("OPENAI_API_KEY"),
    ),
    agent_config={
        "sys_prompt": "You're a helpful assistant named {name}.",
    },
    agent_builder=ReActAgent,
)

print("âœ… AgentScope agent created successfully")


@asynccontextmanager
async def create_runner():
    async with Runner(
        agent=llm_agent,
        context_manager=ContextManager(),
    ) as runner:
        print("âœ… Runneråˆ›å»ºæˆåŠŸ")
        yield runner

async def interact_with_agent(runner):
    # Create a request
    request = AgentRequest(
        input=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "æ³•å›½çš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
                    },
                ],
            },
        ],
    )

    # æµå¼è·å–å“åº”
    print("ğŸ¤– æ™ºèƒ½ä½“æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
    all_result = ""
    async for message in runner.stream_query(request=request):
        # Check if this is a completed message
        if (
            message.object == "message"
            and MessageType.MESSAGE == message.type
            and RunStatus.Completed == message.status
        ):
            all_result = message.content[0].text

    print(f"ğŸ“æ™ºèƒ½ä½“å›å¤: {all_result}")
    return all_result


async def test_interaction():
    async with create_runner() as runner:
        await interact_with_agent(runner)


from agentscope_runtime.engine.deployers import LocalDeployManager

async def deploy_agent(runner):
    # åˆ›å»ºéƒ¨ç½²ç®¡ç†å™¨
    deploy_manager = LocalDeployManager(
        host="localhost",
        port=8090,
    )

    # å°†æ™ºèƒ½ä½“éƒ¨ç½²ä¸ºæµå¼æœåŠ¡
    deploy_result = await runner.deploy(
        deploy_manager=deploy_manager,
        endpoint_path="/process",
        stream=True,  # Enable streaming responses
    )
    print(f"ğŸš€æ™ºèƒ½ä½“éƒ¨ç½²åœ¨: {deploy_result}")
    print(f"ğŸŒæœåŠ¡URL: {deploy_manager.service_url}")
    print(f"ğŸ’š å¥åº·æ£€æŸ¥: {deploy_manager.service_url}/health")

    return deploy_manager

async def run_deployment():
    async with create_runner() as runner:
        deploy_manager = await deploy_agent(runner)

    # Keep the service running (in production, you'd handle this differently)
    print("ğŸƒ Service is running...")

    return deploy_manager

# Deploy the agent
# To deploy from main, use: asyncio.run(run_deployment())
   

if __name__ == "__main__":
    import asyncio
    asyncio.run(run_deployment())